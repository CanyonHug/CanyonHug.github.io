---
title: "DQN architecture"
excerpt: "what DQN architecture is"
date: 2023-04-15
categories:
    - CITD 1
    - AI
tags:
    - RL
last_modified_at: 2023-04-15
---
- basic concepts of DQN and some techniques used in DQN


# 1. Q-learning
Q-learning is a **model-free** RL method. 

The family of 'Q-learning' learn an **action-value function** $Q_\theta (s, a)$ that approximates the optimal action-value function $Q^* (s, a)$. In general, they use a target function based on **the Bellman equation**. (usually done in **off-policy**) 

It uses the following implicit policy.  

$$ a(s) = arg \, \max_{a} Q_\theta (s, a) $$

Even if Q-learning does not require a model and hence has **fewer assumptions** than the closed-form sollutions, traditional RL techniques are limitted by **the curse of dimensionality**. (The Q-table grows enormously.)

![classic Q-learning](/assets/images/Q-learning.png){: width="80%" height="80%" .align-center}

- **The Bellman Equation**  
  The Bellman equation is a recursive method of calculating the Q-value.

  $$ Q(S_t, a_t) = R + \gamma * Q(S_{t + 1}, a_{t+1}) $$

- **epsilon-greedy method**
  At each step on Q-learning, the agent has 2 options for choosing next action.  
  - take the action **with highest Q-value**  
  - take the action **at random**

  If the agent always chooses the next behavior based on the Q-value, it only repeats selecting the behavior with **high immediate rewards in a short period of time**. Occasionally, **random action** helps agents escape these **suboptimal conditions**.

  1. define a **scalar value (=epsilon)** **between 0 and 1** for **each episode**  
  2. **each time** in a given episode, the agent **generates a random number** between 0 and 1.  
  3. If the generated number is **less than the predefined epsilon value**, the agent **randomly selects the next behavior** from the next available behavior set,  
  **otherwise** selects the **behavior with the highest Q-value**.

  > usually, epsilon is defined as **1 at first episode** and **linearly decrease** as the next episodes progress.  
  $\rightarrow$ It is good for the agent to **explore various options in the beginning**.

  After completing the entire Q-learning loop, a **Q-value table** is obtained. If the states and action become complicated, the Q-value table grows enormously.


# 2. DQN  

The **Deep Q-Network (DQN)** combines the **deep neural network** and the **Q-learning**. It can address the curse of dimensionality challenge faced by Q-learning.

![Deep Q-learning](/assets/images/DQ-learning.png){: width="80%" height="80%" .align-center}

Instead of creating the Q-value table, DQN uses Deep Neural Network(**DNN**) that **outputs Q-values for a given state-action pair**.

The action and state are inputted to the DNN model, and scalar Q-values are output.


# 3. Some techniques for DQN
When working with DNN, we always worked with **independent and identically distributed (i.i.d.)** data samples. However, **in RL**, the **current output affects the next input**.

This means that we work as constantly moving targets and there is a **high correlation between inputs and targets**. DQN solves these problems in two new ways.  

- Using **two seperated DQN**  
- **experience replay buffer**

> These two techniques come from the paper '[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)' by DeepMind Technologies in 2013.

- - -

- Using two seperated DQN (**target network**)  
  The Bellman Equation used in DQN is as follow.

  $$Q(S_t, a_t, \theta) = R + \gamma * Q(S_{t+1}, a_{t+1}, \theta)$$

  $\theta$ means the weight of DQN. $\theta$ is on both the **left and right sides of the equation**.  
  $\Rightarrow$ This means that the **same neural network** is used to obtain the Q-value of the **next state-action pair** as well as the **current state-action pair**. This means that $\theta$ **of every step is updated**, changing both the left and right sides of the next step equation, resulting in **instability in the learning process**, so it is **chasing an unfixed target**.

  It can be seen more clearly by looking at **the loss function** that the DNN is trying to minimize using the gradient descent method.

  $$ L = E[(R + \gamma * Q(S_{t+1}, a_{t+1}, \theta) - Q(S_t, a_t, \theta))^2]$$

  If the exact same network produces Q-values for the current and next state-action pairs, the **two terms of the loss function** continue to change, resulting in a **volatility problem**.

  To solve this problem, DQN uses two networks separated by a **main DNN** and a **target DNN** with the **same architecture**.

  - The **main DNN** calculates the **Q-value of the current state-action pair**  
    $\rightarrow$ the **weights of the main DNN** are **updated at each learning step**
  - The **target DNN** calculates the **Q-value of the next(target) state-action pair**.  
    $\rightarrow$ the **weights of the target DNN** are **fixed**. Each time the **gradient descent** method is **repeated k times**, the **weight of the main network** is **copied to the target network**.

  This method keeps the training process relatively **stable**. The method of **copying the weights** ensures the **prediction accuracy of the target network**.

- - -

- **Experience Replay Buffer**  
  Since the DNN expects **i.i.d. data** as input, it is sufficient to **cache the data in buffer memory** and then **randomly sample the data batch** from the buffer. This batch is then fed to the input of the DNN.

  > **Without buffers**, the DNN receives **highly correlated data**, resulting in **poor optimization** results.

- - -

- **Dueling Networks**  
  The dueling network architecture consists of **two streams of computations**   
  - one stream representing **state values**  
  - the other one representing **action advantages**. 
  
  The two streams are **combined by an aggregator** to output an estimate of the Q function. The dueling architecture **learn the state-value function more efficiently**.

- - -

- **Noisy Nets**  
  **Noisy nets** are proposed to **address the limitations of the $\epsilon$-greedy exploration policy** in vanilla DQN. 
  
  A **linear layer** with a **deterministic and noisy stream** replaces the standard linear $y = b + W x$. 
  
  The noisy net enables the network to **learn to ignore the noisy stream** and **enhance the effectiveness of exploration**.

  $$ y = (b + Wx) + (b_{noisy} \odot \epsilon^b + (W_{noisy} \odot \epsilon^w)x) $$

- - -

![DQN-architecture](/assets/images/DQN-architecture.png){: width="70%" height="70%" .align-center}

This architecture cosists of '**Dueling Networks**' and '**Noisy Nets**'. 