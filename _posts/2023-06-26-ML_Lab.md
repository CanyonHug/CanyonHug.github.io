---
title: "ML Lab summer intern"
excerpt: "ML Lab 2023-summer intern"
date: 2023-06-26
categories:
    - ML_Lab
tags:
    - DL
    
last_modified_at: 2023-06-26
---
- ML Lab 2023-summer intern

# 1. ML Lab summer intern
I decided to participate in ML Lab summer internship to experience laboratory.

# 2. Convex optimization study
ML Lab where I joined concentrated on optimization among ML research topics. So ML Lab students learn '**Convex Optimization**' subject.

- - -

**Weekly Plan**  
- 07/03~07/07 (week 1): Convex Set and Functions, Duality, Gradient Descent problem solving  
- 07/10~07/14 (week 2): Nesterov Acceleration and Oracle Lower Bounds  
- 07/17~07/21 (week 3): Proximal Gradient Methods  
- 07/24~07/28 (week 4): Stochastic Gradient Methods, Variance Reduced Methods  
- 07/31~08/04 (week 5): Newton’s Methods  
- 08/07~08/11 (week 6): Mirror Descent  
- 08/14~08/18 (week 7): Min-max Optimization  
- 08/21~08/25 (week 8): Nonsmooth Optimization, ADMM Part 1  
- 08/28~09/01 (week 9): Nonsmooth Optimization, ADMM Part 2

- - -
Academic officer suggested some references.  

**References**    
- **Week 1**
  - **Convex Set and Functions**  
    - Constantine Caramanis (UT Austin), Optimization Algorithms: videos 2.1~2.6  
    - Sebastian Bubeck, Convex Optimization: Algorithms and Complexity, p5~12
  - **Duality**  
    - Nisheeth Vishnoi (Yale), Algorithms for Convex Optimization: link Chapter 5  
    - Stephen Boyd (Stanford) and Lieven Vandenberghe (UCLA), Convex Optimization: link Chapter 5  
    - Ryan Tibshirani (CMU, UC Berkeley), Convex Optimization: link Lectures for Theory II: Optimality and Duality
  - **Gradient Descent**  
    - Constantine Caramanis (UT Austin), Optimization Algorithms: videos 3.1~3.4  
    - Sebastian Bubeck, Convex Optimization: Algorithms and Complexity, p35~44, 49-52
- **Week 2: Nesterov Acceleration and Oracle Lower Bounds**
  - Constantine Caramanis (UT Austin), Optimization Algorithms: videos 4.1~4.2
  - Sebastian Bubeck, Convex Optimization: Algorithms and Complexity, p52~57
  - Nisheeth Vishnoi (Yale), Algorithms for Convex Optimization: link Chapter 8
  - Yuxin Chen (Princeton, UPenn), Large-Scale Optimization for Data Science: lecture note
- **Week 3: Proximal Gradient Methods**
  - Constantine Caramanis (UT Austin), Optimization Algorithms: videos 5.1~5.4
  - Yuxin Chen (Princeton, UPenn), Large-Scale Optimization for Data Science: lecture note
- **Week 4: Stochastic Gradient Methods, Variance Reduced Methods**
  - Constantine Caramanis (UT Austin), Optimization Algorithms: videos 6.1~6.5
  - Niao He and Bernd Gärtner (ETH Zurich), Optimization for Data Science: lecture note 1, lecture note 2
- **Week 5: Newton’s Methods**
  - Constantine Caramanis (UT Austin), Optimization Algorithms: videos 7.1~8.2
  - Nisheeth Vishnoi (Yale), Algorithms for Convex Optimization: link Chapter 9
  - Ryan Tibshirani (CMU, UC Berkeley), Convex Optimization: link Lectures for Algorithms II: Second-Order Methods
- **Week 6: Mirror Descent**
  - Sebastian Bubeck, Convex Optimization: Algorithms and Complexity, chapter 4
  - Yuxin Chen (Princeton, UPenn), Large-Scale Optimization for Data Science: lecture note
  - Constantine Caramanis (UT Austin), Optimization Algorithms: videos 5.5~5.11
- **Week 7: Min-max Optimization**
  - Niao He and Bernd Gärtner (ETH Zurich), Optimization for Data Science: lecture note 1, lecture note 2
  - Chi Jin (Princeton), Optimization for Machine Learning: link to notes and videos
  - Suvrit Sra (MIT), Optimization for Machine Learning: lecture note 1, lecture note 2
- **Week 8,9: Nonsmooth Optimization, ADMM**
  - Yuxin Chen (Princeton, UPenn), Large-Scale Optimization for Data Science: lecture note 1, lecture note 2, lecture note 3
  - Constantine Caramanis (UT Austin), Optimization Algorithms: videos 11.1~12.5
  - Lieven Vandenberghe (UCLA): ECE236C - Optimization Methods for Large-Scale Systems: lectures 6~12


# 3. Selecting Research Topic  
To proceed some research project, I should find out specific research topic. It would be related in optimization. Maybe it's about optimization in federated learning.   
At first, I was interested in meta-learning. However, the proffessor advise that meta-learning is too broad so as it is first time to experience of researching, I might be going to follow what professor suggested.

- 2023.06.26. : Professor Namhoon Lee suggested the subject '**compressing client models in federated learning**'.  