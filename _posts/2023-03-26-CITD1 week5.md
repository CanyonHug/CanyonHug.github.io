---
title: "CITD1 Week5"
excerpt: "CITD1 Week5 summary"
date: 2023-03-26
categories:
    - CITD 1
tags:
    - RL
    - Cryptocurrency
last_modified_at: 2023-03-26
---
- Convergence IT Design 1 course Week5 summary


# 1. Project plan (Week5 plan)

Weekly plan of the project

- Week 2, 3 : set a specific goal and study basic concepts of cryptocurrency and RL
- Week 3 : Investigating previous research (stock trading AI, other cryptocurrency trading AI)
- Week 4 : **Designing candidate RL models**  
- **Week 5, 6 : Implementing and learning RL models**   
- Week 7 : Identifying Model Issues
- Week 8 : midterm
- Week 9, 10 : Solve model issues, improve performance
- Week 11, 12 : Model Completion and Performance Analysis

The weekly plan was delayed for a week.  
I should design RL model.

# 2. Designing RL model  

![RL 2 icon](/assets/images/RL-2.png){: width="30%" height="30%" .align-center}

**Setting States and Rewards** is the most important one. It **induces the agent to behave the way we want**.

- - -

- **States** : vector to describe the current status of the environment 'cryptocurrency market'  

  - remaining inventory, elapsed time  
  - 5 bid / ask prices, bid/ask spread  
  - 20 SMA(simple moving arrange), EMA

  > states : **input nodes** of DQN 

- - -

- **Actions** : making bids or asks 

  - quantity = 0, 0.2 * TWAP, ~ , 2 * TWAP  
  - price = best bid price ~ (best price - alpha)  
    $\rightarrow$ alpha is a hyperparameter.
  
  (quantity * price) = 11 * 5   

  > actions : **output nodes** of DQN

- - -

- **Rewards** : the reward given by the environment as a kind of feedback

  - for Zero-ending inventory constraint:  
    if 'elapsed time' == 0 and 'remaining inventory' > 0:  
    reward -= ('remaining inventory') * beta  

  - for executed price:  
    reward -= (executed price - low price) * k 

  $\rightarrow$ 'beta' and 'k' are hyperparameters.

- - -

DQN architecture (Double DQN, Dueling network, Noise net)

![DQN architecture](/assets/images/DQN-architecture.png){: width="80%" height="60%" .align-center}

We **choose the action** based on the **Q values**, which is the **output of the DQN**.

After taking action, the DQN agent will get reward and observe next state from the **simulated LOB environment**.  
$\rightarrow$ For reinforcement learning, **OpenAI Gymnasium** was mainly used. (It was Gym before, and it is the **toolkit for developing and comparing RL algorithms**.)

# 3. Simulator planning

![OpenAI Gym](/assets/images/OpenAI-Gym.png){: width="80%" height="80%" .align-center}

I will make a simulator on OpenAI gym using pytorch, binance API.

> We need a simulator or an environment for learning the RL agent, using previous data.

# 4. Next week's plan (Week 6) 

- **making a simulator (learning environment for the agent)**  

> To practice the gymnasium toolkit doing other tasks would be helpful.

